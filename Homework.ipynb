{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf8b07b-4ac7-4dd3-821f-195b4e9ab1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db6cab4f-5472-4a36-a9fd-fb798a38333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb2a07e-840d-45b3-91b8-1407c5c3e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types\n",
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10e5d51-d5c7-48a7-984b-0be6eb234249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version - 3.5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('process-taxi-data') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Q1 spark version\n",
    "print(f\"Spark Version - {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ea2811-1172-4cd3-915b-643d1fc7081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading - D:/data_engenering/week4/fhvhv/2019/fhv_tripdata_2019-10.csv\n",
      "Creating partitions - 6 folder D:/data_engenering/week4/fhvhv/partitions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])\n",
    "#  Load the dataframe\n",
    "file_path = 'D:/data_engenering/week4/fhvhv/2019/fhv_tripdata_2019-10.csv'\n",
    "print(f\"Reading - {file_path}\")\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(file_path)\n",
    "\n",
    "#  Partition the data frame\n",
    "folder_path = 'D:/data_engenering/week4/fhvhv/partitions'\n",
    "print(f\"Creating partitions - 6 folder {folder_path}\")\n",
    "df.head()\n",
    "df = df.repartition(6)\n",
    "df.write.mode('overwrite').parquet(folder_path, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2c4bebb-b86f-4285-8a38-d76afd5f403c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average size of Parquet files: 4.31 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the directory containing Parquet files\n",
    "directory = 'D:/data_engenering/week4/fhvhv/partitions'\n",
    "\n",
    "# List all Parquet files in the directory\n",
    "parquet_files = [file for file in os.listdir(directory) if file.endswith('.parquet')]\n",
    "\n",
    "total_size_bytes = 0\n",
    "\n",
    "# Iterate over each Parquet file\n",
    "for file in parquet_files:\n",
    "    # Get the file path\n",
    "    file_path = os.path.join(directory, file)\n",
    "    \n",
    "    # Get the size of the file in bytes\n",
    "    file_size_bytes = os.path.getsize(file_path)\n",
    "    \n",
    "    # Add the file size to the total siz\n",
    "    total_size_bytes += file_size_bytes\n",
    "\n",
    "# Calculate the average size in MB\n",
    "average_size_mb = total_size_bytes / len(parquet_files) / (1024 * 1024)\n",
    "\n",
    "print(f'Average size of Parquet files: {average_size_mb:.2f} MB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85676e89-aaee-48fe-8821-c6e384a784f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of taxi trips on the 15th of October: 62295\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TaxiTripCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the directory containing Parquet files\n",
    "directory = 'D:/data_engenering/week4/fhvhv/partitions'\n",
    "\n",
    "# Read Parquet files into DataFrame\n",
    "df = spark.read.parquet(directory)\n",
    "\n",
    "# Filter data to include only trips on the 15th of October\n",
    "df_filtered = df.filter(col(\"pickup_datetime\").between(\"2019-10-15 00:00:00\", \"2019-10-15 23:59:59\"))\n",
    "\n",
    "# Count the number of records\n",
    "trip_count = df_filtered.count()\n",
    "\n",
    "print(f\"Number of taxi trips on the 15th of October: {trip_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "649b03fa-6537-4825-93e8-0aa7c6d8febb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of taxi trips on 2019-10-15: 62295\n"
     ]
    }
   ],
   "source": [
    "pickup_dt = '2019-10-15'\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Convert pickup_datetime to date\n",
    "df = df.withColumn('pickup_date', F.to_date(df.pickup_datetime))\n",
    "\n",
    "# Filter data for the given pickup date and count the number of records\n",
    "trip_count = df.filter(F.col('pickup_date') == pickup_dt).count()\n",
    "\n",
    "print(f\"Number of taxi trips on {pickup_dt}: {trip_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4901980-2e2b-4f87-9d14-372165028bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62295|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using SQL syntax\n",
    "df.createOrReplaceTempView('fhvhv_tripdata')\n",
    "pickup_dt = '2019-10-15'\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhvhv_tripdata\n",
    "WHERE\n",
    "    to_date(pickup_datetime) = '{pickup_dt}'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06480d66-a212-4d7b-9a57-ecf51719bf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_trips|\n",
      "+-----------+\n",
      "|      62295|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    count(*) AS total_trips\n",
    "FROM\n",
    "    fhvhv_tripdata\n",
    "WHERE pickup_datetime BETWEEN '2019-10-15 00:00:00' AND '2019-10-15 23:59:59'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7095f0b-09a9-4118-8d81-077e537ffe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+---------------+----------------+------------+------------+-------+----------------------+-----------+\n",
      "|hvfhs_license_num|dispatching_base_num|pickup_datetime|dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|pickup_date|\n",
      "+-----------------+--------------------+---------------+----------------+------------+------------+-------+----------------------+-----------+\n",
      "+-----------------+--------------------+---------------+----------------+------------+------------+-------+----------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM \n",
    "    fhvhv_tripdata\n",
    "WHERE\n",
    "    to_date(dropoff_datetime) is not null\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0173015-1e9c-4bbe-b78c-2cfa24e555b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|pickup_date|max(duration)|\n",
      "+-----------+-------------+\n",
      "| 2019-11-01|         NULL|\n",
      "| 2019-10-05|         NULL|\n",
      "| 2019-10-24|         NULL|\n",
      "| 2019-10-01|         NULL|\n",
      "| 2019-10-22|         NULL|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "['hvfhs_license_num',\n",
    " 'dispatching_base_num',\n",
    " 'pickup_datetime',\n",
    " 'dropoff_datetime',\n",
    " 'PULocationID',\n",
    " 'DOLocationID',\n",
    " 'SR_Flag']\n",
    "df \\\n",
    "    .withColumn('duration', (df.dropoff_datetime.cast('long') - df.pickup_datetime.cast('long'))/( 60 * 60 )) \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .groupBy('pickup_date') \\\n",
    "        .max('duration') \\\n",
    "    .orderBy('max(duration)', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bef7b946-e49a-48fe-a64d-29c737829c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_date|          duration|\n",
      "+-----------+------------------+\n",
      "| 2019-10-01| 335.9661111111111|\n",
      "| 2019-10-02|311.99833333333333|\n",
      "| 2019-10-03|             288.0|\n",
      "| 2019-10-04|             264.0|\n",
      "| 2019-10-05|             240.0|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    to_date(pickup_datetime) AS pickup_date,\n",
    "    MAX((UNIX_TIMESTAMP(COALESCE(dropoff_datetime, '2019-10-15 00:00:00')) - UNIX_TIMESTAMP(pickup_datetime)) / (60 * 60)) AS duration\n",
    "FROM \n",
    "    fhvhv_tripdata\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "482d6b91-2558-4c51-92c0-6f3d77c40487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading - D:/data_engenering/week4/fhvhv/2019/taxi_zone_lookup.csv\n",
      "Creating partitions - 6 folder D:/data_engenering/week4/fhvhv/zons\n"
     ]
    }
   ],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('LocationID', types.IntegerType(), True),\n",
    "    types.StructField('Borough', types.StringType(), True),\n",
    "    types.StructField('Zone', types.StringType(), True),\n",
    "    types.StructField('service_zone', types.StringType(), True)\n",
    "])\n",
    "#  Load the dataframe\n",
    "file_path = 'D:/data_engenering/week4/fhvhv/2019/taxi_zone_lookup.csv'\n",
    "print(f\"Reading - {file_path}\")\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(file_path)\n",
    "\n",
    "#  Partition the data frame\n",
    "folder_path = 'D:/data_engenering/week4/fhvhv/zons'\n",
    "print(f\"Creating partitions - 6 folder {folder_path}\")\n",
    "df.head()\n",
    "df = df.repartition(6)\n",
    "df.write.mode('overwrite').parquet(folder_path, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b11c760-0429-410d-8132-6a6ee2b8554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"zones_data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the directory containing \n",
    "directory = 'D:/data_engenering/week4/fhvhv/zons'\n",
    "\n",
    "# Read Parquet files into DataFrame\n",
    "df = spark.read.parquet(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "438fc844-c726-4c86-abd2-c06a7cb39e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('zones_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4385a3d-5c4f-41a9-b940-cea6819ea747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Zone|\n",
      "+--------------------+\n",
      "|Governor's Island...|\n",
      "|         Westerleigh|\n",
      "|Charleston/Totten...|\n",
      "|Heartland Village...|\n",
      "|       Dyker Heights|\n",
      "|     Jackson Heights|\n",
      "|             Bayside|\n",
      "|      Yorkville West|\n",
      "|Flushing Meadows-...|\n",
      "|Riverdale/North R...|\n",
      "|  Stuyvesant Heights|\n",
      "|Upper West Side N...|\n",
      "|Upper East Side N...|\n",
      "|       Prospect Park|\n",
      "|       Starrett City|\n",
      "|Long Island City/...|\n",
      "|        Bloomingdale|\n",
      "|        Midtown East|\n",
      "|Downtown Brooklyn...|\n",
      "|Saint George/New ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "   Zone\n",
    "FROM \n",
    "    zones_data  \n",
    "GROUP BY \n",
    "    1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6396171f-5b74-4d34-9a0e-a88ab2caa1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Group by 'Zone' and calculate the maximum duration for each zone\n",
    "max_duration_per_zone = df.groupBy('Zone')\n",
    "\n",
    "# Order by max duration in descending order, limit to top 5, and show the result\n",
    "#max_duration_per_zone.orderBy('max_duration', ascending=False).limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdb9e882-c578-401f-b79d-41f295ea9474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroupedData[grouping expressions: [Zone], value: [LocationID: int, Borough: string ... 2 more fields], type: GroupBy]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_duration_per_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3aa68ce-d0fc-4b2d-84b5-244ed9112a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        Zone|Total|\n",
      "+------------+-----+\n",
      "|East Chelsea| 2391|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "   pul.Zone,\n",
    "   COUNT(1) as Total\n",
    "FROM \n",
    "    fhvhv_tripdata fhv \n",
    "    INNER JOIN zones_data pul ON fhv.PULocationID = pul.LocationID  \n",
    "WHERE pul.Zone = \"East Chelsea\"\n",
    "GROUP BY \n",
    "    1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c56b02d-6beb-4155-abab-bf392c13c0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       Zone|Total|\n",
      "+-----------+-----+\n",
      "|Jamaica Bay|   14|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "   pul.Zone,\n",
    "   COUNT(1) as Total\n",
    "FROM \n",
    "    fhvhv_tripdata fhv \n",
    "    INNER JOIN zones_data pul ON fhv.PULocationID = pul.LocationID  \n",
    "WHERE pul.Zone = \"Jamaica Bay\"\n",
    "GROUP BY \n",
    "    1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb6828ff-53bf-44d3-bc59-2fb870c8a519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    Zone|Total|\n",
      "+--------+-----+\n",
      "|Union Sq| 2102|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "   pul.Zone,\n",
    "   COUNT(1) as Total\n",
    "FROM \n",
    "    fhvhv_tripdata fhv \n",
    "    INNER JOIN zones_data pul ON fhv.PULocationID = pul.LocationID  \n",
    "WHERE pul.Zone = \"Union Sq\"\n",
    "GROUP BY \n",
    "    1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6c3c11d-8fe1-4a63-a139-039bc786cb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|               Zone|Total|\n",
      "+-------------------+-----+\n",
      "|Crown Heights North|15701|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "   pul.Zone,\n",
    "   COUNT(1) as Total\n",
    "FROM \n",
    "    fhvhv_tripdata fhv \n",
    "    INNER JOIN zones_data pul ON fhv.PULocationID = pul.LocationID  \n",
    "WHERE pul.Zone = \"Crown Heights North\"\n",
    "GROUP BY \n",
    "    1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58188c3e-388f-41ef-8789-aaa8a504c1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
