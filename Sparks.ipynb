{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"C:\\spark\\spark-3.5.0-bin-hadoop3\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wc' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhvhv_tripdata_2021-01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|   NULL|\n",
      "|           HV0003|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|   NULL|\n",
      "|           HV0003|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|   NULL|\n",
      "|           HV0004|              B02800|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|   NULL|\n",
      "|           HV0004|              B02800|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|   NULL|\n",
      "|           HV0003|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|   NULL|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:33:44', dropoff_datetime='2021-01-01 00:49:07', PULocationID='230', DOLocationID='166', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:55:19', dropoff_datetime='2021-01-01 01:18:21', PULocationID='152', DOLocationID='167', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:23:56', dropoff_datetime='2021-01-01 00:38:05', PULocationID='233', DOLocationID='142', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:42:51', dropoff_datetime='2021-01-01 00:45:50', PULocationID='142', DOLocationID='143', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:48:14', dropoff_datetime='2021-01-01 01:08:42', PULocationID='143', DOLocationID='78', SR_Flag=None)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first 1001 lines from the file\n",
    "with open('fhvhv_tripdata_2021-01.csv', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()[:1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the lines to a new file\n",
    "with open('head.csv', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('SR_Flag', DoubleType(), True)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 33, 44), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 49, 7), PULocationID=230, DOLocationID=166, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 55, 19), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 18, 21), PULocationID=152, DOLocationID=167, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 23, 56), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 38, 5), PULocationID=233, DOLocationID=142, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 42, 51), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 45, 50), PULocationID=142, DOLocationID=143, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 48, 14), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 8, 42), PULocationID=143, DOLocationID=78, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 6, 59), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 43, 1), PULocationID=88, DOLocationID=42, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 50), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 4, 57), PULocationID=42, DOLocationID=151, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 14, 30), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 50, 27), PULocationID=71, DOLocationID=226, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02875', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 22, 54), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 30, 20), PULocationID=112, DOLocationID=255, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02875', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 40, 12), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 53, 31), PULocationID=255, DOLocationID=232, SR_Flag=None)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet('zones')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 24 partitions in our dataframe\n",
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquetize and write to fhvhv/2021/01/ folder\n",
    "df.write.mode(\"overwrite\").parquet('D:/data_engenering/week4/fhvhv/2021/01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('D:/data_engenering/week4/fhvhv/2021/01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df1 = df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2 = df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID').filter(df.hvfhs_license_num == 'HV0003')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and User Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-02|  2021-01-02|           7|         223|\n",
      "| 2021-01-03|  2021-01-03|         254|         167|\n",
      "| 2021-01-04|  2021-01-04|          74|          41|\n",
      "| 2021-01-04|  2021-01-04|         142|         116|\n",
      "| 2021-01-03|  2021-01-03|          20|         174|\n",
      "| 2021-01-06|  2021-01-06|         161|         246|\n",
      "| 2021-01-01|  2021-01-01|         129|         258|\n",
      "| 2021-01-03|  2021-01-03|         162|          79|\n",
      "| 2021-01-05|  2021-01-05|         143|         132|\n",
      "| 2021-01-01|  2021-01-01|         197|         205|\n",
      "| 2021-01-01|  2021-01-01|          61|          62|\n",
      "| 2021-01-04|  2021-01-04|         263|          41|\n",
      "| 2021-01-02|  2021-01-02|         258|          77|\n",
      "| 2021-01-03|  2021-01-03|          61|         181|\n",
      "| 2021-01-05|  2021-01-05|          86|         117|\n",
      "| 2021-01-03|  2021-01-03|          61|         225|\n",
      "| 2021-01-05|  2021-01-05|         164|         141|\n",
      "| 2021-01-02|  2021-01-02|         132|          79|\n",
      "| 2021-01-05|  2021-01-05|         250|          32|\n",
      "| 2021-01-02|  2021-01-02|          70|         223|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A crazy function that changes values when they're divisible by 7 or 3\n",
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'\n",
    "\n",
    "# Creating the actual UDF\n",
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  e/9ce| 2021-01-02|  2021-01-02|           7|         223|\n",
      "|  s/b3d| 2021-01-03|  2021-01-03|         254|         167|\n",
      "|  s/b13| 2021-01-04|  2021-01-04|          74|          41|\n",
      "|  e/9ce| 2021-01-04|  2021-01-04|         142|         116|\n",
      "|  e/b47| 2021-01-03|  2021-01-03|          20|         174|\n",
      "|  s/acd| 2021-01-06|  2021-01-06|         161|         246|\n",
      "|  e/9ce| 2021-01-01|  2021-01-01|         129|         258|\n",
      "|  e/b38| 2021-01-03|  2021-01-03|         162|          79|\n",
      "|  a/b49| 2021-01-05|  2021-01-05|         143|         132|\n",
      "|  e/a39| 2021-01-01|  2021-01-01|         197|         205|\n",
      "|  e/9ce| 2021-01-01|  2021-01-01|          61|          62|\n",
      "|  e/9ce| 2021-01-04|  2021-01-04|         263|          41|\n",
      "|  e/b38| 2021-01-02|  2021-01-02|         258|          77|\n",
      "|  e/9ce| 2021-01-03|  2021-01-03|          61|         181|\n",
      "|  e/9ce| 2021-01-05|  2021-01-05|          86|         117|\n",
      "|  e/9ce| 2021-01-03|  2021-01-03|          61|         225|\n",
      "|  e/9ce| 2021-01-05|  2021-01-05|         164|         141|\n",
      "|  s/b44| 2021-01-02|  2021-01-02|         132|          79|\n",
      "|  e/9ce| 2021-01-05|  2021-01-05|         250|          32|\n",
      "|  e/b48| 2021-01-02|  2021-01-02|          70|         223|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter taxi type (e.g., yellow):  green\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "import os\n",
    "import requests\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_schema = types.StructType([\n",
    "    types.StructField(\"VendorID\", types.IntegerType(), True),\n",
    "    types.StructField(\"lpep_pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"lpep_dropoff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"store_and_fwd_flag\", types.StringType(), True),\n",
    "    types.StructField(\"RatecodeID\", types.IntegerType(), True),\n",
    "    types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"passenger_count\", types.IntegerType(), True),\n",
    "    types.StructField(\"trip_distance\", types.DoubleType(), True),\n",
    "    types.StructField(\"fare_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"extra\", types.DoubleType(), True),\n",
    "    types.StructField(\"mta_tax\", types.DoubleType(), True),\n",
    "    types.StructField(\"tip_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"tolls_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"ehail_fee\", types.DoubleType(), True),\n",
    "    types.StructField(\"improvement_surcharge\", types.DoubleType(), True),\n",
    "    types.StructField(\"total_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"payment_type\", types.IntegerType(), True),\n",
    "    types.StructField(\"trip_type\", types.IntegerType(), True),\n",
    "    types.StructField(\"congestion_surcharge\", types.DoubleType(), True)\n",
    "])\n",
    "\n",
    "yellow_schema = types.StructType([\n",
    "    types.StructField(\"VendorID\", types.IntegerType(), True),\n",
    "    types.StructField(\"tpep_pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"tpep_dropoff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"passenger_count\", types.IntegerType(), True),\n",
    "    types.StructField(\"trip_distance\", types.DoubleType(), True),\n",
    "    types.StructField(\"RatecodeID\", types.IntegerType(), True),\n",
    "    types.StructField(\"store_and_fwd_flag\", types.StringType(), True),\n",
    "    types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"payment_type\", types.IntegerType(), True),\n",
    "    types.StructField(\"fare_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"extra\", types.DoubleType(), True),\n",
    "    types.StructField(\"mta_tax\", types.DoubleType(), True),\n",
    "    types.StructField(\"tip_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"tolls_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"improvement_surcharge\", types.DoubleType(), True),\n",
    "    types.StructField(\"total_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"congestion_surcharge\", types.DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"D:/data_engenering/week4/green2020\"\n",
    "TAXI_TYPE = input(\"Enter taxi type (e.g., yellow): \")\n",
    "YEAR = input(\"Enter year (e.g., 2020): \")\n",
    "\n",
    "URL_PREFIX = \"https://s3.amazonaws.com/nyc-tlc/trip+data\"\n",
    "\n",
    "for MONTH in range(1, 13):\n",
    "    FMONTH = str(MONTH).zfill(2)\n",
    "\n",
    "    URL = f\"{URL_PREFIX}/{TAXI_TYPE}_tripdata_{YEAR}-{FMONTH}.csv\"\n",
    "\n",
    "    LOCAL_PREFIX = f\"data/raw/{TAXI_TYPE}/{YEAR}/{FMONTH}\"\n",
    "    LOCAL_FILE = f\"D:/data_engenering/week4/green2020/{TAXI_TYPE}_tripdata_{YEAR}_{FMONTH}.csv\"\n",
    "    LOCAL_PATH = os.path.join(LOCAL_PREFIX, LOCAL_FILE)\n",
    "\n",
    "    print(f\"Downloading {URL} to {LOCAL_PATH}\")\n",
    "    os.makedirs(LOCAL_PREFIX, exist_ok=True)\n",
    "    response = requests.get(URL)\n",
    "    with open(LOCAL_PATH, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"Compressing {LOCAL_PATH}\")\n",
    "    with open(LOCAL_PATH, 'rb') as f_in:\n",
    "        with gzip.open(LOCAL_PATH + '.gz', 'wb') as f_out:\n",
    "            f_out.writelines(f_in)\n",
    "            \n",
    "    # Read data into DataFrame and write to Parquet\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .schema(schema) \\\n",
    "            .csv(local_path)\n",
    "        \n",
    "        df.repartition(4).write.mode(\"overwrite\").parquet(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output paths\n",
    "output_path = \"D:/data_engenering/week4/green2020\"\n",
    "URL_PREFIX = \"https://s3.amazonaws.com/nyc-tlc/trip+data\"\n",
    "# Define function to download and process data\n",
    "def download_and_process_data(TAXI_TYPE, YEAR, schema,folder):\n",
    "    for MONTH in range(1, 13):\n",
    "        \n",
    "        FMONTH = str(MONTH).zfill(2)\n",
    "    \n",
    "        URL = f\"{URL_PREFIX}/{TAXI_TYPE}_tripdata_{YEAR}-{FMONTH}.csv\"\n",
    "    \n",
    "        LOCAL_PREFIX = f\"data/raw/{TAXI_TYPE}/{YEAR}/{FMONTH}\"\n",
    "        LOCAL_FILE = f\"D:/data_engenering/week4/{folder}/{TAXI_TYPE}_tripdata_{YEAR}_{FMONTH}.csv\"\n",
    "        LOCAL_PATH = os.path.join(LOCAL_PREFIX, LOCAL_FILE)\n",
    "    \n",
    "        print(f\"Downloading {URL} to {LOCAL_PATH}\")\n",
    "        os.makedirs(LOCAL_PREFIX, exist_ok=True)\n",
    "        response = requests.get(URL)\n",
    "        with open(LOCAL_PATH, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    \n",
    "        print(f\"Compressing {LOCAL_PATH}\")\n",
    "        with open(LOCAL_PATH, 'rb') as f_in:\n",
    "            with gzip.open(LOCAL_PATH + '.gz', 'wb') as f_out:\n",
    "                f_out.writelines(f_in)\n",
    "\n",
    "        # Read data into DataFrame and write to Parquet\n",
    "        #df = spark.read \\\n",
    "         #   .option(\"header\", \"true\") \\\n",
    "         #   .schema(schema) \\\n",
    "         #   .csv(LOCAL_PATH)\n",
    "        \n",
    "       # df.repartition(4).write.mode(\"overwrite\").parquet(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2020-01.csv to D:/data_engenering/week4/green2020/green_tripdata_2020_01.csv\n",
      "Compressing D:/data_engenering/week4/green2020/green_tripdata_2020_01.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2020-02.csv to D:/data_engenering/week4/green2020/green_tripdata_2020_02.csv\n",
      "Compressing D:/data_engenering/week4/green2020/green_tripdata_2020_02.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2020-03.csv to D:/data_engenering/week4/green2020/green_tripdata_2020_03.csv\n",
      "Compressing D:/data_engenering/week4/green2020/green_tripdata_2020_03.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2020-04.csv to D:/data_engenering/week4/green2020/green_tripdata_2020_04.csv\n",
      "Compressing D:/data_engenering/week4/green2020/green_tripdata_2020_04.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2020-05.csv to D:/data_engenering/week4/green2020/green_tripdata_2020_05.csv\n",
      "Compressing D:/data_engenering/week4/green2020/green_tripdata_2020_05.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2020-06.csv to D:/data_engenering/week4/green2020/green_tripdata_2020_06.csv\n",
      "Compressing D:/data_engenering/week4/green2020/green_tripdata_2020_06.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2020-07.csv to D:/data_engenering/week4/green2020/green_tripdata_2020_07.csv\n",
      "Compressing D:/data_engenering/week4/green2020/green_tripdata_2020_07.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2020-08.csv to D:/data_engenering/week4/green2020/green_tripdata_2020_08.csv\n",
      "Compressing D:/data_engenering/week4/green2020/green_tripdata_2020_08.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2020-09.csv to D:/data_engenering/week4/green2020/green_tripdata_2020_09.csv\n",
      "Compressing D:/data_engenering/week4/green2020/green_tripdata_2020_09.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2020-10.csv to D:/data_engenering/week4/green2020/green_tripdata_2020_10.csv\n",
      "Compressing D:/data_engenering/week4/green2020/green_tripdata_2020_10.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2020-11.csv to D:/data_engenering/week4/green2020/green_tripdata_2020_11.csv\n",
      "Compressing D:/data_engenering/week4/green2020/green_tripdata_2020_11.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2020-12.csv to D:/data_engenering/week4/green2020/green_tripdata_2020_12.csv\n",
      "Compressing D:/data_engenering/week4/green2020/green_tripdata_2020_12.csv\n"
     ]
    }
   ],
   "source": [
    "# Download and process data for green taxis\n",
    "download_and_process_data(\"green\", \"2020\", green_schema,\"green2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-01.csv to D:/data_engenering/week4/green2021/green_tripdata_2021_01.csv\n",
      "Compressing D:/data_engenering/week4/green2021/green_tripdata_2021_01.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-02.csv to D:/data_engenering/week4/green2021/green_tripdata_2021_02.csv\n",
      "Compressing D:/data_engenering/week4/green2021/green_tripdata_2021_02.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-03.csv to D:/data_engenering/week4/green2021/green_tripdata_2021_03.csv\n",
      "Compressing D:/data_engenering/week4/green2021/green_tripdata_2021_03.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-04.csv to D:/data_engenering/week4/green2021/green_tripdata_2021_04.csv\n",
      "Compressing D:/data_engenering/week4/green2021/green_tripdata_2021_04.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-05.csv to D:/data_engenering/week4/green2021/green_tripdata_2021_05.csv\n",
      "Compressing D:/data_engenering/week4/green2021/green_tripdata_2021_05.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-06.csv to D:/data_engenering/week4/green2021/green_tripdata_2021_06.csv\n",
      "Compressing D:/data_engenering/week4/green2021/green_tripdata_2021_06.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-07.csv to D:/data_engenering/week4/green2021/green_tripdata_2021_07.csv\n",
      "Compressing D:/data_engenering/week4/green2021/green_tripdata_2021_07.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-08.csv to D:/data_engenering/week4/green2021/green_tripdata_2021_08.csv\n",
      "Compressing D:/data_engenering/week4/green2021/green_tripdata_2021_08.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-09.csv to D:/data_engenering/week4/green2021/green_tripdata_2021_09.csv\n",
      "Compressing D:/data_engenering/week4/green2021/green_tripdata_2021_09.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-10.csv to D:/data_engenering/week4/green2021/green_tripdata_2021_10.csv\n",
      "Compressing D:/data_engenering/week4/green2021/green_tripdata_2021_10.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-11.csv to D:/data_engenering/week4/green2021/green_tripdata_2021_11.csv\n",
      "Compressing D:/data_engenering/week4/green2021/green_tripdata_2021_11.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-12.csv to D:/data_engenering/week4/green2021/green_tripdata_2021_12.csv\n",
      "Compressing D:/data_engenering/week4/green2021/green_tripdata_2021_12.csv\n"
     ]
    }
   ],
   "source": [
    "# Download and process data for green taxis\n",
    "download_and_process_data(\"green\", \"2021\", green_schema,\"green2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-01.csv to D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_01.csv\n",
      "Compressing D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_01.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-02.csv to D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_02.csv\n",
      "Compressing D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_02.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-03.csv to D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_03.csv\n",
      "Compressing D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_03.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-04.csv to D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_04.csv\n",
      "Compressing D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_04.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-05.csv to D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_05.csv\n",
      "Compressing D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_05.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-06.csv to D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_06.csv\n",
      "Compressing D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_06.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-07.csv to D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_07.csv\n",
      "Compressing D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_07.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-08.csv to D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_08.csv\n",
      "Compressing D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_08.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-09.csv to D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_09.csv\n",
      "Compressing D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_09.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-10.csv to D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_10.csv\n",
      "Compressing D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_10.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-11.csv to D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_11.csv\n",
      "Compressing D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_11.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-12.csv to D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_12.csv\n",
      "Compressing D:/data_engenering/week4/yellow2020/yellow_tripdata_2020_12.csv\n"
     ]
    }
   ],
   "source": [
    "# Download and process data for yellow taxis\n",
    "download_and_process_data(\"yellow\", \"2020\", yellow_schema,\"yellow2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv to D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_01.csv\n",
      "Compressing D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_01.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-02.csv to D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_02.csv\n",
      "Compressing D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_02.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-03.csv to D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_03.csv\n",
      "Compressing D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_03.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-04.csv to D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_04.csv\n",
      "Compressing D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_04.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-05.csv to D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_05.csv\n",
      "Compressing D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_05.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-06.csv to D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_06.csv\n",
      "Compressing D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_06.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-07.csv to D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_07.csv\n",
      "Compressing D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_07.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-08.csv to D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_08.csv\n",
      "Compressing D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_08.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-09.csv to D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_09.csv\n",
      "Compressing D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_09.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-10.csv to D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_10.csv\n",
      "Compressing D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_10.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-11.csv to D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_11.csv\n",
      "Compressing D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_11.csv\n",
      "Downloading https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-12.csv to D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_12.csv\n",
      "Compressing D:/data_engenering/week4/yellow2021/yellow_tripdata_2021_12.csv\n"
     ]
    }
   ],
   "source": [
    "# Download and process data for yellow taxis\n",
    "download_and_process_data(\"yellow\", \"2021\", yellow_schema,\"yellow2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/1\n",
      "processing data for 2020/2\n",
      "processing data for 2020/3\n",
      "processing data for 2020/4\n",
      "processing data for 2020/5\n",
      "processing data for 2020/6\n",
      "processing data for 2020/7\n",
      "processing data for 2020/8\n",
      "processing data for 2020/9\n",
      "processing data for 2020/10\n",
      "processing data for 2020/11\n",
      "processing data for 2020/12\n"
     ]
    }
   ],
   "source": [
    "year = 2020\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'D:/data_engenering/week4/green2020/green_tripdata_{year}_{month:02d}.csv'\n",
    "    output_path = f'D:/data_engenering/week4/green/{year}/{month:02d}/'\n",
    "\n",
    "    df_green = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(green_schema) \\\n",
    "        .csv(input_path)\n",
    "\n",
    "    df_green \\\n",
    "        .repartition(4) \\\n",
    "        .write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/1\n",
      "processing data for 2021/2\n",
      "processing data for 2021/3\n",
      "processing data for 2021/4\n",
      "processing data for 2021/5\n",
      "processing data for 2021/6\n",
      "processing data for 2021/7\n",
      "processing data for 2021/8\n",
      "processing data for 2021/9\n",
      "processing data for 2021/10\n",
      "processing data for 2021/11\n",
      "processing data for 2021/12\n"
     ]
    }
   ],
   "source": [
    "year = 2021\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'D:/data_engenering/week4/green2021/green_tripdata_{year}_{month:02d}.csv'\n",
    "    output_path = f'D:/data_engenering/week4/green/{year}/{month:02d}/'\n",
    "\n",
    "    df_green = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(green_schema) \\\n",
    "        .csv(input_path)\n",
    "\n",
    "    df_green \\\n",
    "        .repartition(4) \\\n",
    "        .write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/1\n",
      "processing data for 2020/2\n",
      "processing data for 2020/3\n",
      "processing data for 2020/4\n",
      "processing data for 2020/5\n",
      "processing data for 2020/6\n",
      "processing data for 2020/7\n",
      "processing data for 2020/8\n",
      "processing data for 2020/9\n",
      "processing data for 2020/10\n",
      "processing data for 2020/11\n",
      "processing data for 2020/12\n"
     ]
    }
   ],
   "source": [
    "year = 2020\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'D:/data_engenering/week4/yellow2020/yellow_tripdata_{year}_{month:02d}.csv'\n",
    "    output_path = f'D:/data_engenering/week4/yellow/{year}/{month:02d}/'\n",
    "\n",
    "    df_green = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(yellow_schema) \\\n",
    "        .csv(input_path)\n",
    "\n",
    "    df_green \\\n",
    "        .repartition(4) \\\n",
    "        .write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/1\n",
      "processing data for 2021/2\n",
      "processing data for 2021/3\n",
      "processing data for 2021/4\n",
      "processing data for 2021/5\n",
      "processing data for 2021/6\n",
      "processing data for 2021/7\n",
      "processing data for 2021/8\n",
      "processing data for 2021/9\n",
      "processing data for 2021/10\n",
      "processing data for 2021/11\n",
      "processing data for 2021/12\n"
     ]
    }
   ],
   "source": [
    "year = 2021\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'D:/data_engenering/week4/yellow2021/yellow_tripdata_{year}_{month:02d}.csv'\n",
    "    output_path = f'D:/data_engenering/week4/yellow/{year}/{month:02d}/'\n",
    "\n",
    "    df_green = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(yellow_schema) \\\n",
    "        .csv(input_path)\n",
    "\n",
    "    df_green \\\n",
    "        .repartition(4) \\\n",
    "        .write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.parquet('D:/data_engenering/week4/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = df_green \\\n",
    "    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet('D:/data_engenering/week4/yellow/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = df_yellow \\\n",
    "    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DOLocationID',\n",
       " 'PULocationID',\n",
       " 'RatecodeID',\n",
       " 'VendorID',\n",
       " 'congestion_surcharge',\n",
       " 'dropoff_datetime',\n",
       " 'extra',\n",
       " 'fare_amount',\n",
       " 'improvement_surcharge',\n",
       " 'mta_tax',\n",
       " 'passenger_count',\n",
       " 'payment_type',\n",
       " 'pickup_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'total_amount',\n",
       " 'trip_distance'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_green.columns) & set(df_yellow.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_colums = []\n",
    "\n",
    "yellow_columns = set(df_yellow.columns)\n",
    "\n",
    "for col in df_green.columns:\n",
    "    if col in yellow_columns:\n",
    "        common_colums.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_green_sel = df_green \\\n",
    "    .select(common_colums) \\\n",
    "    .withColumn('service_type', F.lit('green'))\n",
    "\n",
    "df_yellow_sel = df_yellow \\\n",
    "    .select(common_colums) \\\n",
    "    .withColumn('service_type', F.lit('yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, pickup_datetime: timestamp, dropoff_datetime: timestamp, store_and_fwd_flag: string, RatecodeID: int, PULocationID: int, DOLocationID: int, passenger_count: int, trip_distance: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, payment_type: int, congestion_surcharge: double, service_type: string]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_green_sel.registerTempTable('green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_sel.registerTempTable('yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+------------+\n",
      "|VendorID|pickup_datetime|dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|congestion_surcharge|service_type|\n",
      "+--------+---------------+----------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+------------+\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "+--------+---------------+----------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "   *\n",
    "FROM\n",
    "    green\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_data = df_green_sel.unionAll(df_yellow_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|service_type|count|\n",
      "+------------+-----+\n",
      "|       green|   24|\n",
      "|      yellow|   24|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trips_data.groupBy('service_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_data.registerTempTable('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|service_type|count(1)|\n",
      "+------------+--------+\n",
      "|       green|      24|\n",
      "|      yellow|      24|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    service_type,\n",
    "    count(1)\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY \n",
    "    service_type\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_trips|\n",
      "+-----------+\n",
      "|          0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    count(*) AS total_trips\n",
    "FROM\n",
    "    trips_data\n",
    "WHERE pickup_datetime BETWEEN '2019-10-15 00:00:00' AND '2019-10-15 23:59:59'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+------------+\n",
      "|VendorID|pickup_datetime|dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|congestion_surcharge|service_type|\n",
      "+--------+---------------+----------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+------------+\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "|    NULL|           NULL|            NULL|              NULL|      NULL|        NULL|        NULL|           NULL|         NULL|       NULL| NULL|   NULL|      NULL|        NULL|                 NULL|        NULL|        NULL|                NULL|       green|\n",
      "+--------+---------------+----------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "   *\n",
    "FROM\n",
    "    trips_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    -- Reveneue grouping \n",
    "    PULocationID AS revenue_zone,\n",
    "    date_trunc('month', pickup_datetime) AS revenue_month, \n",
    "    service_type, \n",
    "\n",
    "    -- Revenue calculation \n",
    "    SUM(fare_amount) AS revenue_monthly_fare,\n",
    "    SUM(extra) AS revenue_monthly_extra,\n",
    "    SUM(mta_tax) AS revenue_monthly_mta_tax,\n",
    "    SUM(tip_amount) AS revenue_monthly_tip_amount,\n",
    "    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n",
    "    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n",
    "    SUM(total_amount) AS revenue_monthly_total_amount,\n",
    "    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n",
    "\n",
    "    -- Additional calculations\n",
    "    AVG(passenger_count) AS avg_montly_passenger_count,\n",
    "    AVG(trip_distance) AS avg_montly_trip_distance\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY\n",
    "    1, 2, 3\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[revenue_zone: int, revenue_month: timestamp, service_type: string, revenue_monthly_fare: double, revenue_monthly_extra: double, revenue_monthly_mta_tax: double, revenue_monthly_tip_amount: double, revenue_monthly_tolls_amount: double, revenue_monthly_improvement_surcharge: double, revenue_monthly_total_amount: double, revenue_monthly_congestion_surcharge: double, avg_montly_passenger_count: double, avg_montly_trip_distance: double]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.write.mode(\"overwrite\").parquet('D:/data_engenering/week4/data/report/revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.coalesce(1).write.mode(\"overwrite\").parquet('D:/data_engenering/week4/data/report/revenue', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    yellow\n",
    "WHERE\n",
    "    pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+--------------+\n",
      "|hour|zone|amount|number_records|\n",
      "+----+----+------+--------------+\n",
      "+----+----+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green_revenue.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue_tmp = df_green_revenue \\\n",
    "    .withColumnRenamed('amount', 'green_amount') \\\n",
    "    .withColumnRenamed('number_records', 'green_number_records')\n",
    "\n",
    "df_yellow_revenue_tmp = df_yellow_revenue \\\n",
    "    .withColumnRenamed('amount', 'yellow_amount') \\\n",
    "    .withColumnRenamed('number_records', 'yellow_number_records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_green_revenue_tmp.join(df_yellow_revenue_tmp, on=['hour', 'zone'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------------+--------------------+-------------+---------------------+\n",
      "|hour|zone|green_amount|green_number_records|yellow_amount|yellow_number_records|\n",
      "+----+----+------------+--------------------+-------------+---------------------+\n",
      "+----+----+------------+--------------------+-------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version - 3.5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('process-taxi-data') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Q1 spark version\n",
    "print(f\"Spark Version - {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-03-03 01:19:26--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240302%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240302T221925Z&X-Amz-Expires=300&X-Amz-Signature=87dbcad51f5969307138c17b3cde72e14f8d2ccc7442913a39125062a164aa71&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-03-03 01:19:27--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240302%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240302T221925Z&X-Amz-Expires=300&X-Amz-Signature=87dbcad51f5969307138c17b3cde72e14f8d2ccc7442913a39125062a164aa71&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19375751 (18M) [application/octet-stream]\n",
      "Saving to: 'fhv_tripdata_2019-10.csv.gz'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0%  204K 93s\n",
      "    50K .......... .......... .......... .......... ..........  0%  624K 61s\n",
      "   100K .......... .......... .......... .......... ..........  0%  852K 48s\n",
      "   150K .......... .......... .......... .......... ..........  1%  477K 46s\n",
      "   200K .......... .......... .......... .......... ..........  1%  554K 43s\n",
      "   250K .......... .......... .......... .......... ..........  1%  832K 40s\n",
      "   300K .......... .......... .......... .......... ..........  1%  820K 37s\n",
      "   350K .......... .......... .......... .......... ..........  2%  816K 35s\n",
      "   400K .......... .......... .......... .......... ..........  2% 72.6K 60s\n",
      "   450K .......... .......... .......... .......... ..........  2%  294M 53s\n",
      "   500K .......... .......... .......... .......... ..........  2%  249M 48s\n",
      "   550K .......... .......... .......... .......... ..........  3%  198M 44s\n",
      "   600K .......... .......... .......... .......... ..........  3%  309M 41s\n",
      "   650K .......... .......... .......... .......... ..........  3% 6.07M 38s\n",
      "   700K .......... .......... .......... .......... ..........  3%  184M 35s\n",
      "   750K .......... .......... .......... .......... ..........  4% 6.94M 33s\n",
      "   800K .......... .......... .......... .......... ..........  4%  134K 39s\n",
      "   850K .......... .......... .......... .......... ..........  4% 2.42M 37s\n",
      "   900K .......... .......... .......... .......... ..........  5% 21.7M 35s\n",
      "   950K .......... .......... .......... .......... ..........  5%  144M 33s\n",
      "  1000K .......... .......... .......... .......... ..........  5% 1.92M 32s\n",
      "  1050K .......... .......... .......... .......... ..........  5%  126K 37s\n",
      "  1100K .......... .......... .......... .......... ..........  6% 99.3M 35s\n",
      "  1150K .......... .......... .......... .......... ..........  6%  253K 37s\n",
      "  1200K .......... .......... .......... .......... ..........  6% 26.5M 35s\n",
      "  1250K .......... .......... .......... .......... ..........  6%  225M 34s\n",
      "  1300K .......... .......... .......... .......... ..........  7% 1.81M 33s\n",
      "  1350K .......... .......... .......... .......... ..........  7%  831K 32s\n",
      "  1400K .......... .......... .......... .......... ..........  7%  817K 32s\n",
      "  1450K .......... .......... .......... .......... ..........  7%  825K 31s\n",
      "  1500K .......... .......... .......... .......... ..........  8%  820K 31s\n",
      "  1550K .......... .......... .......... .......... ..........  8%  830K 30s\n",
      "  1600K .......... .......... .......... .......... ..........  8%  609K 30s\n",
      "  1650K .......... .......... .......... .......... ..........  8%  726K 30s\n",
      "  1700K .......... .......... .......... .......... ..........  9%  832K 30s\n",
      "  1750K .......... .......... .......... .......... ..........  9%  789K 29s\n",
      "  1800K .......... .......... .......... .......... ..........  9%  765K 29s\n",
      "  1850K .......... .......... .......... .......... .......... 10%  773K 29s\n",
      "  1900K .......... .......... .......... .......... .......... 10%  779K 29s\n",
      "  1950K .......... .......... .......... .......... .......... 10%  778K 28s\n",
      "  2000K .......... .......... .......... .......... .......... 10%  163K 30s\n",
      "  2050K .......... .......... .......... .......... .......... 11% 1.63M 30s\n",
      "  2100K .......... .......... .......... .......... .......... 11%  180M 29s\n",
      "  2150K .......... .......... .......... .......... .......... 11%  224M 28s\n",
      "  2200K .......... .......... .......... .......... .......... 11% 5.75M 27s\n",
      "  2250K .......... .......... .......... .......... .......... 12%  788K 27s\n",
      "  2300K .......... .......... .......... .......... .......... 12%  756K 27s\n",
      "  2350K .......... .......... .......... .......... .......... 12%  700K 27s\n",
      "  2400K .......... .......... .......... .......... .......... 12%  605K 27s\n",
      "  2450K .......... .......... .......... .......... .......... 13%  801K 27s\n",
      "  2500K .......... .......... .......... .......... .......... 13%  787K 26s\n",
      "  2550K .......... .......... .......... .......... .......... 13%  760K 26s\n",
      "  2600K .......... .......... .......... .......... .......... 14%  761K 26s\n",
      "  2650K .......... .......... .......... .......... .......... 14%  200K 27s\n",
      "  2700K .......... .......... .......... .......... .......... 14% 1.64M 27s\n",
      "  2750K .......... .......... .......... .......... .......... 14%  344M 26s\n",
      "  2800K .......... .......... .......... .......... .......... 15%  315K 26s\n",
      "  2850K .......... .......... .......... .......... .......... 15%  288M 26s\n",
      "  2900K .......... .......... .......... .......... .......... 15%  388M 25s\n",
      "  2950K .......... .......... .......... .......... .......... 15% 1.23M 25s\n",
      "  3000K .......... .......... .......... .......... .......... 16%  866K 25s\n",
      "  3050K .......... .......... .......... .......... .......... 16%  765K 25s\n",
      "  3100K .......... .......... .......... .......... .......... 16%  774K 25s\n",
      "  3150K .......... .......... .......... .......... .......... 16%  763K 24s\n",
      "  3200K .......... .......... .......... .......... .......... 17%  156K 26s\n",
      "  3250K .......... .......... .......... .......... .......... 17% 1.22M 25s\n",
      "  3300K .......... .......... .......... .......... .......... 17% 17.9M 25s\n",
      "  3350K .......... .......... .......... .......... .......... 17%  220M 24s\n",
      "  3400K .......... .......... .......... .......... .......... 18%  167M 24s\n",
      "  3450K .......... .......... .......... .......... .......... 18%  867K 24s\n",
      "  3500K .......... .......... .......... .......... .......... 18%  806K 24s\n",
      "  3550K .......... .......... .......... .......... .......... 19%  762K 23s\n",
      "  3600K .......... .......... .......... .......... .......... 19% 77.3K 26s\n",
      "  3650K .......... .......... .......... .......... .......... 19%  697K 26s\n",
      "  3700K .......... .......... .......... .......... .......... 19%  172K 26s\n",
      "  3750K .......... .......... .......... .......... .......... 20%  105M 26s\n",
      "  3800K .......... .......... .......... .......... .......... 20%  256M 26s\n",
      "  3850K .......... .......... .......... .......... .......... 20%  346M 25s\n",
      "  3900K .......... .......... .......... .......... .......... 20%  228K 26s\n",
      "  3950K .......... .......... .......... .......... .......... 21%  170K 26s\n",
      "  4000K .......... .......... .......... .......... .......... 21%  173M 26s\n",
      "  4050K .......... .......... .......... .......... .......... 21%  375M 25s\n",
      "  4100K .......... .......... .......... .......... .......... 21%  409M 25s\n",
      "  4150K .......... .......... .......... .......... .......... 22%  185M 25s\n",
      "  4200K .......... .......... .......... .......... .......... 22%  434M 24s\n",
      "  4250K .......... .......... .......... .......... .......... 22% 1.29M 24s\n",
      "  4300K .......... .......... .......... .......... .......... 22%  467K 24s\n",
      "  4350K .......... .......... .......... .......... .......... 23% 2.07M 24s\n",
      "  4400K .......... .......... .......... .......... .......... 23%  491K 24s\n",
      "  4450K .......... .......... .......... .......... .......... 23% 15.2K 34s\n",
      "  4500K .......... .......... .......... .......... .......... 24%  212K 34s\n",
      "  4550K .......... .......... .......... .......... .......... 24%  137M 34s\n",
      "  4600K .......... .......... .......... .......... .......... 24%  137M 33s\n",
      "  4650K .......... .......... .......... .......... .......... 24% 2.81M 33s\n",
      "  4700K .......... .......... .......... .......... .......... 25%  249K 33s\n",
      "  4750K .......... .......... .......... .......... .......... 25%  235K 33s\n",
      "  4800K .......... .......... .......... .......... .......... 25%  476K 33s\n",
      "  4850K .......... .......... .......... .......... .......... 25%  253K 33s\n",
      "  4900K .......... .......... .......... .......... .......... 26%  535K 33s\n",
      "  4950K .......... .......... .......... .......... .......... 26%  496K 33s\n",
      "  5000K .......... .......... .......... .......... .......... 26%  565K 33s\n",
      "  5050K .......... .......... .......... .......... .......... 26%  237K 33s\n",
      "  5100K .......... .......... .......... .......... .......... 27% 4.38M 32s\n",
      "  5150K .......... .......... .......... .......... .......... 27% 1.61M 32s\n",
      "  5200K .......... .......... .......... .......... .......... 27%  184K 32s\n",
      "  5250K .......... .......... .......... .......... .......... 28%  198M 32s\n",
      "  5300K .......... .......... .......... .......... .......... 28% 22.2M 31s\n",
      "  5350K .......... .......... .......... .......... .......... 28%  531K 31s\n",
      "  5400K .......... .......... .......... .......... .......... 28%  293K 31s\n",
      "  5450K .......... .......... .......... .......... .......... 29%  234K 31s\n",
      "  5500K .......... .......... .......... .......... .......... 29%  148M 31s\n",
      "  5550K .......... .......... .......... .......... .......... 29% 1.07M 31s\n",
      "  5600K .......... .......... .......... .......... .......... 29%  463K 31s\n",
      "  5650K .......... .......... .......... .......... .......... 30%  610K 30s\n",
      "  5700K .......... .......... .......... .......... .......... 30%  653K 30s\n",
      "  5750K .......... .......... .......... .......... .......... 30%  679K 30s\n",
      "  5800K .......... .......... .......... .......... .......... 30%  579K 30s\n",
      "  5850K .......... .......... .......... .......... .......... 31%  600K 30s\n",
      "  5900K .......... .......... .......... .......... .......... 31%  631K 29s\n",
      "  5950K .......... .......... .......... .......... .......... 31%  647K 29s\n",
      "  6000K .......... .......... .......... .......... .......... 31%  506K 29s\n",
      "  6050K .......... .......... .......... .......... .......... 32%  548K 29s\n",
      "  6100K .......... .......... .......... .......... .......... 32%  729K 29s\n",
      "  6150K .......... .......... .......... .......... .......... 32%  609K 29s\n",
      "  6200K .......... .......... .......... .......... .......... 33%  576K 28s\n",
      "  6250K .......... .......... .......... .......... .......... 33%  634K 28s\n",
      "  6300K .......... .......... .......... .......... .......... 33%  565K 28s\n",
      "  6350K .......... .......... .......... .......... .......... 33%  666K 28s\n",
      "  6400K .......... .......... .......... .......... .......... 34%  494K 28s\n",
      "  6450K .......... .......... .......... .......... .......... 34%  525K 28s\n",
      "  6500K .......... .......... .......... .......... .......... 34%  772K 27s\n",
      "  6550K .......... .......... .......... .......... .......... 34%  537K 27s\n",
      "  6600K .......... .......... .......... .......... .......... 35%  707K 27s\n",
      "  6650K .......... .......... .......... .......... .......... 35%  654K 27s\n",
      "  6700K .......... .......... .......... .......... .......... 35%  279K 27s\n",
      "  6750K .......... .......... .......... .......... .......... 35%  209M 27s\n",
      "  6800K .......... .......... .......... .......... .......... 36%  450K 26s\n",
      "  6850K .......... .......... .......... .......... .......... 36%  471K 26s\n",
      "  6900K .......... .......... .......... .......... .......... 36%  723K 26s\n",
      "  6950K .......... .......... .......... .......... .......... 36%  395K 26s\n",
      "  7000K .......... .......... .......... .......... .......... 37%  838K 26s\n",
      "  7050K .......... .......... .......... .......... .......... 37%  771K 26s\n",
      "  7100K .......... .......... .......... .......... .......... 37%  737K 26s\n",
      "  7150K .......... .......... .......... .......... .......... 38%  648K 25s\n",
      "  7200K .......... .......... .......... .......... .......... 38%  345K 25s\n",
      "  7250K .......... .......... .......... .......... .......... 38%  376K 25s\n",
      "  7300K .......... .......... .......... .......... .......... 38%  379K 25s\n",
      "  7350K .......... .......... .......... .......... .......... 39%  766K 25s\n",
      "  7400K .......... .......... .......... .......... .......... 39%  774K 25s\n",
      "  7450K .......... .......... .......... .......... .......... 39%  891K 25s\n",
      "  7500K .......... .......... .......... .......... .......... 39%  449K 25s\n",
      "  7550K .......... .......... .......... .......... .......... 40%  379K 24s\n",
      "  7600K .......... .......... .......... .......... .......... 40%  134K 25s\n",
      "  7650K .......... .......... .......... .......... .......... 40%  245M 25s\n",
      "  7700K .......... .......... .......... .......... .......... 40%  330M 24s\n",
      "  7750K .......... .......... .......... .......... .......... 41%  202M 24s\n",
      "  7800K .......... .......... .......... .......... .......... 41% 1.43M 24s\n",
      "  7850K .......... .......... .......... .......... .......... 41%  808K 24s\n",
      "  7900K .......... .......... .......... .......... .......... 42%  510K 23s\n",
      "  7950K .......... .......... .......... .......... .......... 42%  565K 23s\n",
      "  8000K .......... .......... .......... .......... .......... 42%  625K 23s\n",
      "  8050K .......... .......... .......... .......... .......... 42%  508K 23s\n",
      "  8100K .......... .......... .......... .......... .......... 43%  124K 23s\n",
      "  8150K .......... .......... .......... .......... .......... 43% 2.73M 23s\n",
      "  8200K .......... .......... .......... .......... .......... 43%  194M 23s\n",
      "  8250K .......... .......... .......... .......... .......... 43%  229M 23s\n",
      "  8300K .......... .......... .......... .......... .......... 44%  145M 22s\n",
      "  8350K .......... .......... .......... .......... .......... 44% 1.17M 22s\n",
      "  8400K .......... .......... .......... .......... .......... 44%  202K 22s\n",
      "  8450K .......... .......... .......... .......... .......... 44%  240M 22s\n",
      "  8500K .......... .......... .......... .......... .......... 45%  168M 22s\n",
      "  8550K .......... .......... .......... .......... .......... 45% 3.06M 22s\n",
      "  8600K .......... .......... .......... .......... .......... 45%  822K 21s\n",
      "  8650K .......... .......... .......... .......... .......... 45%  824K 21s\n",
      "  8700K .......... .......... .......... .......... .......... 46%  805K 21s\n",
      "  8750K .......... .......... .......... .......... .......... 46%  833K 21s\n",
      "  8800K .......... .......... .......... .......... .......... 46%  605K 21s\n",
      "  8850K .......... .......... .......... .......... .......... 47%  789K 21s\n",
      "  8900K .......... .......... .......... .......... .......... 47%  691K 21s\n",
      "  8950K .......... .......... .......... .......... .......... 47%  704K 20s\n",
      "  9000K .......... .......... .......... .......... .......... 47%  798K 20s\n",
      "  9050K .......... .......... .......... .......... .......... 48%  828K 20s\n",
      "  9100K .......... .......... .......... .......... .......... 48%  823K 20s\n",
      "  9150K .......... .......... .......... .......... .......... 48%  812K 20s\n",
      "  9200K .......... .......... .......... .......... .......... 48%  617K 20s\n",
      "  9250K .......... .......... .......... .......... .......... 49%  821K 20s\n",
      "  9300K .......... .......... .......... .......... .......... 49%  831K 19s\n",
      "  9350K .......... .......... .......... .......... .......... 49%  775K 19s\n",
      "  9400K .......... .......... .......... .......... .......... 49%  787K 19s\n",
      "  9450K .......... .......... .......... .......... .......... 50%  872K 19s\n",
      "  9500K .......... .......... .......... .......... .......... 50%  743K 19s\n",
      "  9550K .......... .......... .......... .......... .......... 50%  931K 19s\n",
      "  9600K .......... .......... .......... .......... .......... 50%  604K 19s\n",
      "  9650K .......... .......... .......... .......... .......... 51%  816K 18s\n",
      "  9700K .......... .......... .......... .......... .......... 51%  839K 18s\n",
      "  9750K .......... .......... .......... .......... .......... 51%  817K 18s\n",
      "  9800K .......... .......... .......... .......... .......... 52%  854K 18s\n",
      "  9850K .......... .......... .......... .......... .......... 52%  791K 18s\n",
      "  9900K .......... .......... .......... .......... .......... 52%  828K 18s\n",
      "  9950K .......... .......... .......... .......... .......... 52%  849K 18s\n",
      " 10000K .......... .......... .......... .......... .......... 53%  618K 18s\n",
      " 10050K .......... .......... .......... .......... .......... 53%  589K 17s\n",
      " 10100K .......... .......... .......... .......... .......... 53% 1007K 17s\n",
      " 10150K .......... .......... .......... .......... .......... 53%  894K 17s\n",
      " 10200K .......... .......... .......... .......... .......... 54%  848K 17s\n",
      " 10250K .......... .......... .......... .......... .......... 54%  853K 17s\n",
      " 10300K .......... .......... .......... .......... .......... 54%  827K 17s\n",
      " 10350K .......... .......... .......... .......... .......... 54%  751K 17s\n",
      " 10400K .......... .......... .......... .......... .......... 55%  329K 17s\n",
      " 10450K .......... .......... .......... .......... .......... 55%  105K 17s\n",
      " 10500K .......... .......... .......... .......... .......... 55%  123M 17s\n",
      " 10550K .......... .......... .......... .......... .......... 56%  247M 16s\n",
      " 10600K .......... .......... .......... .......... .......... 56%  177M 16s\n",
      " 10650K .......... .......... .......... .......... .......... 56%  742K 16s\n",
      " 10700K .......... .......... .......... .......... .......... 56%  135K 16s\n",
      " 10750K .......... .......... .......... .......... .......... 57%  217M 16s\n",
      " 10800K .......... .......... .......... .......... .......... 57%  145M 16s\n",
      " 10850K .......... .......... .......... .......... .......... 57%  204M 16s\n",
      " 10900K .......... .......... .......... .......... .......... 57%  335M 16s\n",
      " 10950K .......... .......... .......... .......... .......... 58% 2.00M 15s\n",
      " 11000K .......... .......... .......... .......... .......... 58%  626K 15s\n",
      " 11050K .......... .......... .......... .......... .......... 58% 1.11M 15s\n",
      " 11100K .......... .......... .......... .......... .......... 58%  815K 15s\n",
      " 11150K .......... .......... .......... .......... .......... 59%  823K 15s\n",
      " 11200K .......... .......... .......... .......... .......... 59%  612K 15s\n",
      " 11250K .......... .......... .......... .......... .......... 59%  832K 15s\n",
      " 11300K .......... .......... .......... .......... .......... 59%  841K 15s\n",
      " 11350K .......... .......... .......... .......... .......... 60%  787K 14s\n",
      " 11400K .......... .......... .......... .......... .......... 60%  820K 14s\n",
      " 11450K .......... .......... .......... .......... .......... 60%  818K 14s\n",
      " 11500K .......... .......... .......... .......... .......... 61%  835K 14s\n",
      " 11550K .......... .......... .......... .......... .......... 61%  191K 14s\n",
      " 11600K .......... .......... .......... .......... .......... 61%  254K 14s\n",
      " 11650K .......... .......... .......... .......... .......... 61%  876K 14s\n",
      " 11700K .......... .......... .......... .......... .......... 62%  820K 14s\n",
      " 11750K .......... .......... .......... .......... .......... 62%  841K 14s\n",
      " 11800K .......... .......... .......... .......... .......... 62%  808K 14s\n",
      " 11850K .......... .......... .......... .......... .......... 62%  206K 14s\n",
      " 11900K .......... .......... .......... .......... .......... 63%  130M 13s\n",
      " 11950K .......... .......... .......... .......... .......... 63%  202M 13s\n",
      " 12000K .......... .......... .......... .......... .......... 63% 2.84M 13s\n",
      " 12050K .......... .......... .......... .......... .......... 63%  186K 13s\n",
      " 12100K .......... .......... .......... .......... .......... 64%  154M 13s\n",
      " 12150K .......... .......... .......... .......... .......... 64%  304M 13s\n",
      " 12200K .......... .......... .......... .......... .......... 64%  186M 13s\n",
      " 12250K .......... .......... .......... .......... .......... 65% 1.37M 13s\n",
      " 12300K .......... .......... .......... .......... .......... 65%  854K 12s\n",
      " 12350K .......... .......... .......... .......... .......... 65%  824K 12s\n",
      " 12400K .......... .......... .......... .......... .......... 65%  614K 12s\n",
      " 12450K .......... .......... .......... .......... .......... 66%  836K 12s\n",
      " 12500K .......... .......... .......... .......... .......... 66%  828K 12s\n",
      " 12550K .......... .......... .......... .......... .......... 66%  716K 12s\n",
      " 12600K .......... .......... .......... .......... .......... 66%  949K 12s\n",
      " 12650K .......... .......... .......... .......... .......... 67%  760K 12s\n",
      " 12700K .......... .......... .......... .......... .......... 67%  896K 12s\n",
      " 12750K .......... .......... .......... .......... .......... 67%  793K 11s\n",
      " 12800K .......... .......... .......... .......... .......... 67%  624K 11s\n",
      " 12850K .......... .......... .......... .......... .......... 68%  832K 11s\n",
      " 12900K .......... .......... .......... .......... .......... 68%  806K 11s\n",
      " 12950K .......... .......... .......... .......... .......... 68%  822K 11s\n",
      " 13000K .......... .......... .......... .......... .......... 68%  823K 11s\n",
      " 13050K .......... .......... .......... .......... .......... 69%  839K 11s\n",
      " 13100K .......... .......... .......... .......... .......... 69%  826K 11s\n",
      " 13150K .......... .......... .......... .......... .......... 69%  817K 11s\n",
      " 13200K .......... .......... .......... .......... .......... 70%  618K 11s\n",
      " 13250K .......... .......... .......... .......... .......... 70%  775K 10s\n",
      " 13300K .......... .......... .......... .......... .......... 70%  879K 10s\n",
      " 13350K .......... .......... .......... .......... .......... 70%  810K 10s\n",
      " 13400K .......... .......... .......... .......... .......... 71%  820K 10s\n",
      " 13450K .......... .......... .......... .......... .......... 71%  832K 10s\n",
      " 13500K .......... .......... .......... .......... .......... 71%  832K 10s\n",
      " 13550K .......... .......... .......... .......... .......... 71%  826K 10s\n",
      " 13600K .......... .......... .......... .......... .......... 72%  577K 10s\n",
      " 13650K .......... .......... .......... .......... .......... 72%  906K 10s\n",
      " 13700K .......... .......... .......... .......... .......... 72%  145K 10s\n",
      " 13750K .......... .......... .......... .......... .......... 72%  323M 9s\n",
      " 13800K .......... .......... .......... .......... .......... 73%  394K 9s\n",
      " 13850K .......... .......... .......... .......... .......... 73%  206M 9s\n",
      " 13900K .......... .......... .......... .......... .......... 73%  212M 9s\n",
      " 13950K .......... .......... .......... .......... .......... 73%  402M 9s\n",
      " 14000K .......... .......... .......... .......... .......... 74% 3.08M 9s\n",
      " 14050K .......... .......... .......... .......... .......... 74%  951K 9s\n",
      " 14100K .......... .......... .......... .......... .......... 74%  844K 9s\n",
      " 14150K .......... .......... .......... .......... .......... 75%  811K 9s\n",
      " 14200K .......... .......... .......... .......... .......... 75%  798K 8s\n",
      " 14250K .......... .......... .......... .......... .......... 75%  863K 8s\n",
      " 14300K .......... .......... .......... .......... .......... 75%  817K 8s\n",
      " 14350K .......... .......... .......... .......... .......... 76%  813K 8s\n",
      " 14400K .......... .......... .......... .......... .......... 76%  615K 8s\n",
      " 14450K .......... .......... .......... .......... .......... 76%  842K 8s\n",
      " 14500K .......... .......... .......... .......... .......... 76%  796K 8s\n",
      " 14550K .......... .......... .......... .......... .......... 77%  830K 8s\n",
      " 14600K .......... .......... .......... .......... .......... 77%  837K 8s\n",
      " 14650K .......... .......... .......... .......... .......... 77%  810K 8s\n",
      " 14700K .......... .......... .......... .......... .......... 77%  794K 7s\n",
      " 14750K .......... .......... .......... .......... .......... 78%  745K 7s\n",
      " 14800K .......... .......... .......... .......... .......... 78%  577K 7s\n",
      " 14850K .......... .......... .......... .......... .......... 78%  930K 7s\n",
      " 14900K .......... .......... .......... .......... .......... 79%  825K 7s\n",
      " 14950K .......... .......... .......... .......... .......... 79%  815K 7s\n",
      " 15000K .......... .......... .......... .......... .......... 79%  666K 7s\n",
      " 15050K .......... .......... .......... .......... .......... 79% 1004K 7s\n",
      " 15100K .......... .......... .......... .......... .......... 80%  710K 7s\n",
      " 15150K .......... .......... .......... .......... .......... 80%  971K 7s\n",
      " 15200K .......... .......... .......... .......... .......... 80%  610K 7s\n",
      " 15250K .......... .......... .......... .......... .......... 80%  798K 6s\n",
      " 15300K .......... .......... .......... .......... .......... 81%  769K 6s\n",
      " 15350K .......... .......... .......... .......... .......... 81%  711K 6s\n",
      " 15400K .......... .......... .......... .......... .......... 81%  491K 6s\n",
      " 15450K .......... .......... .......... .......... .......... 81%  811K 6s\n",
      " 15500K .......... .......... .......... .......... .......... 82%  133K 6s\n",
      " 15550K .......... .......... .......... .......... .......... 82%  742K 6s\n",
      " 15600K .......... .......... .......... .......... .......... 82%  115M 6s\n",
      " 15650K .......... .......... .......... .......... .......... 82%  178M 6s\n",
      " 15700K .......... .......... .......... .......... .......... 83%  228K 6s\n",
      " 15750K .......... .......... .......... .......... .......... 83%  258M 6s\n",
      " 15800K .......... .......... .......... .......... .......... 83%  332M 5s\n",
      " 15850K .......... .......... .......... .......... .......... 84%  325M 5s\n",
      " 15900K .......... .......... .......... .......... .......... 84% 1.44M 5s\n",
      " 15950K .......... .......... .......... .......... .......... 84%  803K 5s\n",
      " 16000K .......... .......... .......... .......... .......... 84%  161K 5s\n",
      " 16050K .......... .......... .......... .......... .......... 85%  197M 5s\n",
      " 16100K .......... .......... .......... .......... .......... 85%  250M 5s\n",
      " 16150K .......... .......... .......... .......... .......... 85%  252M 5s\n",
      " 16200K .......... .......... .......... .......... .......... 85%  613K 5s\n",
      " 16250K .......... .......... .......... .......... .......... 86%  672K 5s\n",
      " 16300K .......... .......... .......... .......... .......... 86%  134K 5s\n",
      " 16350K .......... .......... .......... .......... .......... 86%  218M 4s\n",
      " 16400K .......... .......... .......... .......... .......... 86%  231M 4s\n",
      " 16450K .......... .......... .......... .......... .......... 87%  129M 4s\n",
      " 16500K .......... .......... .......... .......... .......... 87% 7.31M 4s\n",
      " 16550K .......... .......... .......... .......... .......... 87%  716K 4s\n",
      " 16600K .......... .......... .......... .......... .......... 87%  729K 4s\n",
      " 16650K .......... .......... .......... .......... .......... 88%  665K 4s\n",
      " 16700K .......... .......... .......... .......... .......... 88%  739K 4s\n",
      " 16750K .......... .......... .......... .......... .......... 88%  732K 4s\n",
      " 16800K .......... .......... .......... .......... .......... 89%  464K 4s\n",
      " 16850K .......... .......... .......... .......... .......... 89%  766K 4s\n",
      " 16900K .......... .......... .......... .......... .......... 89%  751K 3s\n",
      " 16950K .......... .......... .......... .......... .......... 89%  674K 3s\n",
      " 17000K .......... .......... .......... .......... .......... 90%  744K 3s\n",
      " 17050K .......... .......... .......... .......... .......... 90%  477K 3s\n",
      " 17100K .......... .......... .......... .......... .......... 90%  726K 3s\n",
      " 17150K .......... .......... .......... .......... .......... 90%  665K 3s\n",
      " 17200K .......... .......... .......... .......... .......... 91%  430K 3s\n",
      " 17250K .......... .......... .......... .......... .......... 91%  813K 3s\n",
      " 17300K .......... .......... .......... .......... .......... 91%  854K 3s\n",
      " 17350K .......... .......... .......... .......... .......... 91%  544K 3s\n",
      " 17400K .......... .......... .......... .......... .......... 92%  824K 3s\n",
      " 17450K .......... .......... .......... .......... .......... 92%  527K 2s\n",
      " 17500K .......... .......... .......... .......... .......... 92%  888K 2s\n",
      " 17550K .......... .......... .......... .......... .......... 93%  517K 2s\n",
      " 17600K .......... .......... .......... .......... .......... 93%  485K 2s\n",
      " 17650K .......... .......... .......... .......... .......... 93%  612K 2s\n",
      " 17700K .......... .......... .......... .......... .......... 93%  721K 2s\n",
      " 17750K .......... .......... .......... .......... .......... 94%  142K 2s\n",
      " 17800K .......... .......... .......... .......... .......... 94% 1.65M 2s\n",
      " 17850K .......... .......... .......... .......... .......... 94%  210M 2s\n",
      " 17900K .......... .......... .......... .......... .......... 94%  563K 2s\n",
      " 17950K .......... .......... .......... .......... .......... 95%  187M 2s\n",
      " 18000K .......... .......... .......... .......... .......... 95%  192M 2s\n",
      " 18050K .......... .......... .......... .......... .......... 95%  661K 1s\n",
      " 18100K .......... .......... .......... .......... .......... 95%  912K 1s\n",
      " 18150K .......... .......... .......... .......... .......... 96%  654K 1s\n",
      " 18200K .......... .......... .......... .......... .......... 96%  535K 1s\n",
      " 18250K .......... .......... .......... .......... .......... 96%  658K 1s\n",
      " 18300K .......... .......... .......... .......... .......... 96%  744K 1s\n",
      " 18350K .......... .......... .......... .......... .......... 97%  743K 1s\n",
      " 18400K .......... .......... .......... .......... .......... 97%  609K 1s\n",
      " 18450K .......... .......... .......... .......... .......... 97%  825K 1s\n",
      " 18500K .......... .......... .......... .......... .......... 98%  759K 1s\n",
      " 18550K .......... .......... .......... .......... .......... 98%  912K 1s\n",
      " 18600K .......... .......... .......... .......... .......... 98%  806K 0s\n",
      " 18650K .......... .......... .......... .......... .......... 98%  816K 0s\n",
      " 18700K .......... .......... .......... .......... .......... 99%  821K 0s\n",
      " 18750K .......... .......... .......... .......... .......... 99%  835K 0s\n",
      " 18800K .......... .......... .......... .......... .......... 99%  611K 0s\n",
      " 18850K .......... .......... .......... .......... .......... 99%  134K 0s\n",
      " 18900K .......... .......... .                               100% 2.01M=33s\n",
      "\n",
      "2024-03-03 01:20:01 (574 KB/s) - 'fhv_tripdata_2019-10.csv.gz' saved [19375751/19375751]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading - D:/data_engenering/week4/fhvhv/2019/fhv_tripdata_2019-10.csv\n",
      "Creating partitions - 6 folder D:/data_engenering/week4/fhvhv/partitions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])\n",
    "#  Load the dataframe\n",
    "file_path = 'D:/data_engenering/week4/fhvhv/2019/fhv_tripdata_2019-10.csv'\n",
    "print(f\"Reading - {file_path}\")\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(file_path)\n",
    "\n",
    "#  Partition the data frame\n",
    "folder_path = 'D:/data_engenering/week4/fhvhv/partitions'\n",
    "print(f\"Creating partitions - 6 folder {folder_path}\")\n",
    "df.head()\n",
    "df = df.repartition(6)\n",
    "df.write.mode('overwrite').parquet(folder_path, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir /S /B D:\\data_engenering\\week4\\fhvhv\\partitions\\*.parquet > D:/data_engenering/week4/fhvhv/size.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\data_engenering\\week4\\fhvhv\\partitions\\part-00000-6358915c-23b3-4fbd-a2cb-ea6c0aa4856e-c000.gz.parquet\n",
      "D:\\data_engenering\\week4\\fhvhv\\partitions\\part-00001-6358915c-23b3-4fbd-a2cb-ea6c0aa4856e-c000.gz.parquet\n",
      "D:\\data_engenering\\week4\\fhvhv\\partitions\\part-00002-6358915c-23b3-4fbd-a2cb-ea6c0aa4856e-c000.gz.parquet\n",
      "D:\\data_engenering\\week4\\fhvhv\\partitions\\part-00003-6358915c-23b3-4fbd-a2cb-ea6c0aa4856e-c000.gz.parquet\n",
      "D:\\data_engenering\\week4\\fhvhv\\partitions\\part-00004-6358915c-23b3-4fbd-a2cb-ea6c0aa4856e-c000.gz.parquet\n",
      "D:\\data_engenering\\week4\\fhvhv\\partitions\\part-00005-6358915c-23b3-4fbd-a2cb-ea6c0aa4856e-c000.gz.parquet\n"
     ]
    }
   ],
   "source": [
    "!dir /S /B D:\\data_engenering\\week4\\fhvhv\\partitions\\*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average size of Parquet files: 4.31 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the directory containing Parquet files\n",
    "directory = 'D:/data_engenering/week4/fhvhv/partitions'\n",
    "\n",
    "# List all Parquet files in the directory\n",
    "parquet_files = [file for file in os.listdir(directory) if file.endswith('.parquet')]\n",
    "\n",
    "total_size_bytes = 0\n",
    "\n",
    "# Iterate over each Parquet file\n",
    "for file in parquet_files:\n",
    "    # Get the file path\n",
    "    file_path = os.path.join(directory, file)\n",
    "    \n",
    "    # Get the size of the file in bytes\n",
    "    file_size_bytes = os.path.getsize(file_path)\n",
    "    \n",
    "    # Add the file size to the total siz\n",
    "    total_size_bytes += file_size_bytes\n",
    "\n",
    "# Calculate the average size in MB\n",
    "average_size_mb = total_size_bytes / len(parquet_files) / (1024 * 1024)\n",
    "\n",
    "print(f'Average size of Parquet files: {average_size_mb:.2f} MB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of taxi trips on the 15th of October: 62295\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TaxiTripCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the directory containing Parquet files\n",
    "directory = 'D:/data_engenering/week4/fhvhv/partitions'\n",
    "\n",
    "# Read Parquet files into DataFrame\n",
    "df = spark.read.parquet(directory)\n",
    "\n",
    "# Filter data to include only trips on the 15th of October\n",
    "df_filtered = df.filter(col(\"pickup_datetime\").between(\"2019-10-15 00:00:00\", \"2019-10-15 23:59:59\"))\n",
    "\n",
    "# Count the number of records\n",
    "trip_count = df_filtered.count()\n",
    "\n",
    "print(f\"Number of taxi trips on the 15th of October: {trip_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of taxi trips on 2019-10-15: 62295\n"
     ]
    }
   ],
   "source": [
    "pickup_dt = '2019-10-15'\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Convert pickup_datetime to date\n",
    "df = df.withColumn('pickup_date', F.to_date(df.pickup_datetime))\n",
    "\n",
    "# Filter data for the given pickup date and count the number of records\n",
    "trip_count = df.filter(F.col('pickup_date') == pickup_dt).count()\n",
    "\n",
    "print(f\"Number of taxi trips on {pickup_dt}: {trip_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62295|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using SQL syntax\n",
    "df.createOrReplaceTempView('fhvhv_tripdata')\n",
    "pickup_dt = '2019-10-15'\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhvhv_tripdata\n",
    "WHERE\n",
    "    to_date(pickup_datetime) = '{pickup_dt}'\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_trips|\n",
      "+-----------+\n",
      "|      62295|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    count(*) AS total_trips\n",
    "FROM\n",
    "    fhvhv_tripdata\n",
    "WHERE pickup_datetime BETWEEN '2019-10-15 00:00:00' AND '2019-10-15 23:59:59'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+---------------+----------------+------------+------------+-------+----------------------+-----------+\n",
      "|hvfhs_license_num|dispatching_base_num|pickup_datetime|dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|pickup_date|\n",
      "+-----------------+--------------------+---------------+----------------+------------+------------+-------+----------------------+-----------+\n",
      "+-----------------+--------------------+---------------+----------------+------------+------------+-------+----------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM \n",
    "    fhvhv_tripdata\n",
    "WHERE\n",
    "    to_date(dropoff_datetime) is not null\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|pickup_date|max(duration)|\n",
      "+-----------+-------------+\n",
      "| 2019-11-01|         NULL|\n",
      "| 2019-10-05|         NULL|\n",
      "| 2019-10-24|         NULL|\n",
      "| 2019-10-01|         NULL|\n",
      "| 2019-10-22|         NULL|\n",
      "+-----------+-------------+\n",
      "\n",
      "+-----------+--------+\n",
      "|pickup_date|duration|\n",
      "+-----------+--------+\n",
      "| 2019-11-01|    NULL|\n",
      "| 2019-10-05|    NULL|\n",
      "| 2019-10-24|    NULL|\n",
      "| 2019-10-01|    NULL|\n",
      "| 2019-10-22|    NULL|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "['hvfhs_license_num',\n",
    " 'dispatching_base_num',\n",
    " 'pickup_datetime',\n",
    " 'dropoff_datetime',\n",
    " 'PULocationID',\n",
    " 'DOLocationID',\n",
    " 'SR_Flag']\n",
    "df \\\n",
    "    .withColumn('duration', (df.dropoff_datetime.cast('long') - df.pickup_datetime.cast('long'))/( 60 * 60 )) \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .groupBy('pickup_date') \\\n",
    "        .max('duration') \\\n",
    "    .orderBy('max(duration)', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_date|          duration|\n",
      "+-----------+------------------+\n",
      "| 2019-10-01| 335.9661111111111|\n",
      "| 2019-10-02|311.99833333333333|\n",
      "| 2019-10-03|             288.0|\n",
      "| 2019-10-04|             264.0|\n",
      "| 2019-10-05|             240.0|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    to_date(pickup_datetime) AS pickup_date,\n",
    "    MAX((UNIX_TIMESTAMP(COALESCE(dropoff_datetime, '2019-10-15 00:00:00')) - UNIX_TIMESTAMP(pickup_datetime)) / (60 * 60)) AS duration\n",
    "FROM \n",
    "    fhvhv_tripdata\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading - D:/data_engenering/week4/fhvhv/2019/taxi_zone_lookup.csv\n",
      "Creating partitions - 6 folder D:/data_engenering/week4/fhvhv/zons\n"
     ]
    }
   ],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('LocationID', types.IntegerType(), True),\n",
    "    types.StructField('Borough', types.StringType(), True),\n",
    "    types.StructField('Zone', types.StringType(), True),\n",
    "    types.StructField('service_zone', types.StringType(), True)\n",
    "])\n",
    "#  Load the dataframe\n",
    "file_path = 'D:/data_engenering/week4/fhvhv/2019/taxi_zone_lookup.csv'\n",
    "print(f\"Reading - {file_path}\")\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(file_path)\n",
    "\n",
    "#  Partition the data frame\n",
    "folder_path = 'D:/data_engenering/week4/fhvhv/zons'\n",
    "print(f\"Creating partitions - 6 folder {folder_path}\")\n",
    "df.head()\n",
    "df = df.repartition(6)\n",
    "df.write.mode('overwrite').parquet(folder_path, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"zones_data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the directory containing \n",
    "directory = 'D:/data_engenering/week4/fhvhv/zons'\n",
    "\n",
    "# Read Parquet files into DataFrame\n",
    "df = spark.read.parquet(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('zones_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Zone|\n",
      "+--------------------+\n",
      "|Governor's Island...|\n",
      "|         Westerleigh|\n",
      "|Charleston/Totten...|\n",
      "|Heartland Village...|\n",
      "|       Dyker Heights|\n",
      "|     Jackson Heights|\n",
      "|             Bayside|\n",
      "|      Yorkville West|\n",
      "|Flushing Meadows-...|\n",
      "|Riverdale/North R...|\n",
      "|  Stuyvesant Heights|\n",
      "|Upper West Side N...|\n",
      "|Upper East Side N...|\n",
      "|       Prospect Park|\n",
      "|       Starrett City|\n",
      "|Long Island City/...|\n",
      "|        Bloomingdale|\n",
      "|        Midtown East|\n",
      "|Downtown Brooklyn...|\n",
      "|Saint George/New ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "   Zone\n",
    "FROM \n",
    "    zones_data  \n",
    "GROUP BY \n",
    "    1\n",
    "\"\"\").show()\n",
    "\n",
    "#East Chelsea\n",
    "\n",
    "#Jamaica Bay\n",
    "\n",
    "#Union Sq\n",
    "\n",
    "#Crown Heights North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Group by 'Zone' and calculate the maximum duration for each zone\n",
    "max_duration_per_zone = df.groupBy('Zone')\n",
    "\n",
    "# Order by max duration in descending order, limit to top 5, and show the result\n",
    "#max_duration_per_zone.orderBy('max_duration', ascending=False).limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroupedData[grouping expressions: [Zone], value: [LocationID: int, Borough: string ... 2 more fields], type: GroupBy]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_duration_per_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        Zone|Total|\n",
      "+------------+-----+\n",
      "|East Chelsea| 2391|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "   pul.Zone,\n",
    "   COUNT(1) as Total\n",
    "FROM \n",
    "    fhvhv_tripdata fhv \n",
    "    INNER JOIN zones_data pul ON fhv.PULocationID = pul.LocationID  \n",
    "WHERE pul.Zone = \"East Chelsea\"\n",
    "GROUP BY \n",
    "    1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       Zone|Total|\n",
      "+-----------+-----+\n",
      "|Jamaica Bay|   14|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "   pul.Zone,\n",
    "   COUNT(1) as Total\n",
    "FROM \n",
    "    fhvhv_tripdata fhv \n",
    "    INNER JOIN zones_data pul ON fhv.PULocationID = pul.LocationID  \n",
    "WHERE pul.Zone = \"Jamaica Bay\"\n",
    "GROUP BY \n",
    "    1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    Zone|Total|\n",
      "+--------+-----+\n",
      "|Union Sq| 2102|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "   pul.Zone,\n",
    "   COUNT(1) as Total\n",
    "FROM \n",
    "    fhvhv_tripdata fhv \n",
    "    INNER JOIN zones_data pul ON fhv.PULocationID = pul.LocationID  \n",
    "WHERE pul.Zone = \"Union Sq\"\n",
    "GROUP BY \n",
    "    1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|               Zone|Total|\n",
      "+-------------------+-----+\n",
      "|Crown Heights North|15701|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "   pul.Zone,\n",
    "   COUNT(1) as Total\n",
    "FROM \n",
    "    fhvhv_tripdata fhv \n",
    "    INNER JOIN zones_data pul ON fhv.PULocationID = pul.LocationID  \n",
    "WHERE pul.Zone = \"Crown Heights North\"\n",
    "GROUP BY \n",
    "    1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
